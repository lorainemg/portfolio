[{"categories":["Basic"],"contents":"Applied data extraction techniques to obtain the data, data analysis to visualize and analyze such data, and machine learning algorithms to predict cryptocurrency prices in the short future.\nSummary Created a Streamlit app to display information about cryptocurrencies. Different data of interest were plotted, which are essential for the prediction of future cryptocurrency prices. Twitter comments are extracted using its API, to extract and subsequently classify opinions concerning a cryptocurrency in question. Sentiment analysis is performed using pre-trained models to detect the opinion of a cryptocurrency in a given period of time. Skills Machine Learning Time-Series Forecasting Twitter API Plotly-Dash Streamlit Natural Language Processing Resources GitHub Link ","date":"August 24, 2022","hero":"/portfolio/posts/personal/project4/hero.png","permalink":"https://lorainemg.github.io/portfolio/posts/personal/project4/","summary":"Applied data extraction techniques to obtain the data, data analysis to visualize and analyze such data, and machine learning algorithms to predict cryptocurrency prices in the short future.","tags":["Data Analysis","Data Visualization","Time-Series Analysis","Machine Learning","NLP","API"],"title":"Cryptocurrency Forecasting"},{"categories":["Basic"],"contents":"Summary Applied data extraction techniques to obtain the data, data analysis to visualize and analyze such data, and machine learning algorithms to predict cryptocurrency prices in the short future.\nCreated a Streamlit app to display information about cryptocurrencies. Different data of interest were plotted, which are essential for the prediction of future cryptocurrency prices. Twitter comments are extracted using its API, to extract and subsequently classify opinions concerning a cryptocurrency in question. Sentiment analysis is performed using pre-trained models to detect the opinion of a cryptocurrency in a given period of time. Skills Machine Learning Time-Series Forecasting Twitter API Plotly-Dash Streamlit Natural Language Processing Resources GitHub Link ","date":"August 24, 2022","hero":"/portfolio/projects/personal/project4/hero.png","permalink":"https://lorainemg.github.io/portfolio/projects/personal/project4/","summary":"\u003ch4 id=\"summary\"\u003eSummary\u003c/h4\u003e\n\u003cp\u003eApplied data extraction techniques to obtain the data, data analysis to visualize and analyze such data, and machine learning algorithms to predict cryptocurrency prices in the short future.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCreated a Streamlit app to display information about cryptocurrencies.\u003c/li\u003e\n\u003cli\u003eDifferent data of interest were plotted, which are essential for the prediction of future cryptocurrency prices.\u003c/li\u003e\n\u003cli\u003eTwitter comments are extracted using its API, to extract and subsequently classify opinions concerning a cryptocurrency in question.\u003c/li\u003e\n\u003cli\u003eSentiment analysis is performed using pre-trained models to detect the opinion of a cryptocurrency in a given period of time.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"skills\"\u003e\u003cstrong\u003eSkills\u003c/strong\u003e\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eMachine Learning\u003c/li\u003e\n\u003cli\u003eTime-Series Forecasting\u003c/li\u003e\n\u003cli\u003eTwitter API\u003c/li\u003e\n\u003cli\u003ePlotly-Dash\u003c/li\u003e\n\u003cli\u003eStreamlit\u003c/li\u003e\n\u003cli\u003eNatural Language Processing\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"resources\"\u003eResources\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lorainemg/crypto-forecasting\" target=\"_blank\" rel=\"noopener\"\u003eGitHub Link\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":["Data Analysis","Data Visualization","Time-Series Analysis","Machine Learning","NLP","API"],"title":"Cryptocurrency Forecasting"},{"categories":["Basic"],"contents":"My Collection of Projects at DataCamp. This exercises are taken from the course Associate Data Scientist in Python from DataCamp. Here\u0026rsquo;s a brief description of each project.\nIntroduction to DataCamp Projects This notebook serves as an Introduction to DataCamp Projects.\nIntroduces what a Jupyter notebook is, showing how to run code cells. Show how to load and show pandas DataFrames. Show how to show inline matplotlib plots. Show the interactive output of Maps. Habilities Data Manipulation, Data Visualization, Importing \u0026amp; Cleaning Data, Case Studies Links GitHub Repository Investigating Netflix Movies and Guest Stars in The Office In this project I apply the foundational Python skills by manipulating and visualizing movie and TV data.\nSome data manipulation with Pandas is done. The database is loaded from a csv file. Matplotlib is used to show visualizations of the data. The films are analyzed by genre, to determine if this is correlated with its duration. Habilities Data Manipulation, Data Visualization, Programming, Case Studies Links DataCamp Workspace GitHub Repository The GitHub History of the Scala Language Find the true Scala experts by exploring its development history in Git and GitHub.\nWe read in, clean up, and visualize the real-world project repository of Scala that spans data from a version control system (Git) as well as a project hosting site (GitHub). We find out who has had the most influence on its development and who are the experts. Habilities Data Manipulation, Data Visualization, Importing \u0026amp; Cleaning Data Links DataCamp Workspace GitHub Repository The Android App Market on Google Play Load, clean, and visualize scraped Google Play Store data to gain insights into the Android app market.\nWe do a comprehensive analysis of the Android app market by comparing over ten thousand apps in Google Play across different categories. We look for insights in the data to devise strategies to drive growth. Habilities Data Manipulation, Data Visualization, Probability \u0026amp; Statistics, Importing \u0026amp; Cleaning Data Links DataCamp Workspace GitHub Repository A Visual History of Nobel Prize Winners Explore a dataset from Kaggle containing a century\u0026rsquo;s worth of Nobel Laureates.\nThe dataset is analyzed to look for biased data according to gender and nationality. Other analyses are carried out, including how old the winners of Nobel prizes are, the differences between price categories, the oldest and younger winners, etc. Habilities Data Manipulation, Data Visualization, Importing \u0026amp; Cleaning Data Links DataCamp Workspace GitHub Repository ","date":"August 24, 2022","hero":"/portfolio/posts/work/project8/hero.png","permalink":"https://lorainemg.github.io/portfolio/posts/work/project8/","summary":"My Collection of Projects at DataCamp. This exercises are taken from the course Associate Data Scientist in Python from DataCamp. Here\u0026rsquo;s a brief description of each project.","tags":["Data Manipulation","Data Visualization","Importing \u0026 Cleaning Data","Programming","Probability \u0026 Statistics","Machine Learning"],"title":"DataCamp Projects"},{"categories":["Basic"],"contents":"My Collection of Projects at DataCamp. This exercises are taken from the course Associate Data Scientist in Python from DataCamp. Here\u0026rsquo;s a brief description of each project.\nIntroduction to DataCamp Projects This notebook serves as an Introduction to DataCamp Projects.\nIntroduces what a Jupyter notebook is, showing how to run code cells. Show how to load and show pandas DataFrames. Show how to show inline matplotlib plots. Show the interactive output of Maps. Habilities Data Manipulation, Data Visualization, Importing \u0026amp; Cleaning Data, Case Studies Links GitHub Repository Investigating Netflix Movies and Guest Stars in The Office In this project I apply the foundational Python skills by manipulating and visualizing movie and TV data.\nSome data manipulation with Pandas is done. The database is loaded from a csv file. Matplotlib is used to show visualizations of the data. The films are analyzed by genre, to determine if this is correlated with its duration. Habilities Data Manipulation, Data Visualization, Programming, Case Studies Links DataCamp Workspace GitHub Repository The GitHub History of the Scala Language Find the true Scala experts by exploring its development history in Git and GitHub.\nWe read in, clean up, and visualize the real-world project repository of Scala that spans data from a version control system (Git) as well as a project hosting site (GitHub). We find out who has had the most influence on its development and who are the experts. Habilities Data Manipulation, Data Visualization, Importing \u0026amp; Cleaning Data Links DataCamp Workspace GitHub Repository The Android App Market on Google Play Load, clean, and visualize scraped Google Play Store data to gain insights into the Android app market.\nWe do a comprehensive analysis of the Android app market by comparing over ten thousand apps in Google Play across different categories. We look for insights in the data to devise strategies to drive growth. Habilities Data Manipulation, Data Visualization, Probability \u0026amp; Statistics, Importing \u0026amp; Cleaning Data Links DataCamp Workspace GitHub Repository A Visual History of Nobel Prize Winners Explore a dataset from Kaggle containing a century\u0026rsquo;s worth of Nobel Laureates.\nThe dataset is analyzed to look for biased data according to gender and nationality. Other analyses are carried out, including how old the winners of Nobel prizes are, the differences between price categories, the oldest and younger winners, etc. Habilities Data Manipulation, Data Visualization, Importing \u0026amp; Cleaning Data Links DataCamp Workspace GitHub Repository ","date":"August 24, 2022","hero":"/portfolio/projects/work/project8/hero.png","permalink":"https://lorainemg.github.io/portfolio/projects/work/project8/","summary":"\u003cp\u003eMy Collection of Projects at DataCamp. This exercises are taken from the course \u003ca href=\"https://www.datacamp.com/tracks/associate-data-scientist-in-python\" target=\"_blank\" rel=\"noopener\"\u003eAssociate Data Scientist in Python\u003c/a\u003e from \u003ca href=\"https://www.datacamp.com/\" target=\"_blank\" rel=\"noopener\"\u003eDataCamp\u003c/a\u003e. Here\u0026rsquo;s a brief description of each project.\u003c/p\u003e\n\u003ch4 id=\"introduction-to-datacamp-projects\"\u003eIntroduction to DataCamp Projects\u003c/h4\u003e\n\u003cp\u003eThis notebook serves as an Introduction to DataCamp Projects.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIntroduces what a Jupyter notebook is, showing how to run code cells.\u003c/li\u003e\n\u003cli\u003eShow how to load and show pandas DataFrames.\u003c/li\u003e\n\u003cli\u003eShow how to show inline matplotlib plots.\u003c/li\u003e\n\u003cli\u003eShow the interactive output of Maps.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"habilities\"\u003eHabilities\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eData Manipulation\u003c/code\u003e, \u003ccode\u003eData Visualization\u003c/code\u003e, \u003ccode\u003eImporting \u0026amp; Cleaning Data\u003c/code\u003e, \u003ccode\u003eCase Studies\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"links\"\u003eLinks\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lorainemg/datacamp-projects/tree/main/Introduction%20to%20DataCamp%20Projects\" target=\"_blank\" rel=\"noopener\"\u003eGitHub Repository\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"investigating-netflix-movies-and-guest-stars-in-the-office\"\u003eInvestigating Netflix Movies and Guest Stars in The Office\u003c/h4\u003e\n\u003cp\u003eIn this project I apply the foundational Python skills by manipulating and visualizing movie and TV data.\u003c/p\u003e","tags":["Data Manipulation","Data Visualization","Importing \u0026 Cleaning Data","Programming","Probability \u0026 Statistics","Machine Learning"],"title":"DataCamp Projects"},{"categories":["Basic"],"contents":"My Collection of Projects teaching Machine Learning subject. Here\u0026rsquo;s a brief description of every project:\nSupervised Classification and Introduction to the Sklearn Tool The objective of this project is to solve a problem using the algorithms given in the lecture.\nThere are positive and negative movie reviews. 1000 of each. The goal is, given the text of the review, to determine whether the review is positive or negative. For this, KNN and Naive Bayes will be used and compared. Habilities: Data Preprocessing KNN Naive Bayes Feature Extraction Model Training Sentiment Analysis Decision Trees and Random Forest To demonstrate the use of Decision Trees and Random Forests, we use the Iris dataset.\nThe characteristics of the Iris data set are analyzed, visualizing the most important characteristics. The behavior of each algorithm is analyzed, visualizing its decision space. The depth of the trees is adjusted to avoid overfitting. Then, with the Decision Tree and Random Forest algorithms, the dataset used in the previous project, Rotten Tomatoes, is analyzed, where the sentiment analysis is performed. Habilities: Data Preprocessing Decision Trees Random Forest Feature Analysis Model Tuning Model Training Sentiment Analysis Logistic Regression We continue to use the Rotten Tomatoes dataset to rank positive and negative reviews.\nAs in the previous projects, we extract the content of the files, get a bag of words representation of the dataset, and begin to train the model. This time, we visualize the Confusion Matrix to assess the performance of the algorithm and show how to test the prediction of the algorithm in a specific example. In addition, we use the coefficient of the Logistic Regression model to check the most important features. We plot the most important positive and negative words. We check different techniques that would make the model better: add bigrams and decrease the probability threshold of Logistic Regression. Habilities: Data Preprocessing Logistic Regression Feature Analysis Model Tuning Model Training Sentiment Analysis Natural Language Processing Support Vector Machine and Cross Validation We work with a dataset that contains information about patients (gender, marital status, smoking status, age, etc.) with the aim of predicting whether they are likely to have a heart attack (stroke). In addition, the doctor characteristic is artificially incorporated, which represents the doctor who collected the data, which will later be used to group the data.\nWe clean our data encoding some categorical variables and checking for NaN values. We use Support Vector Machine with this dataset to classify patients that could have a heart attack. AUC metric is introduced, good for the imbalanced datasets, like the one we were using. The concept of Baseline is used, to efficiently compare models and different partition methods to use with Cross Validation are visualized. Different algorithms are compared using correct evaluation techniques. Habilities: Data Cleaning Support Vector Machines Model Training Model Evaluation Cross Validation Clustering Algorithms: K-Means y DBSCAN In this project, we explore how K-means and DBSCAN work.\nWe show various K-means issues and provide solutions to address those issues, including cluster quality analysis. We study how to artificially generate datasets with Sklearn, visualizing the data distribution to apply clustering. K-means algorithm is explained, and the predictions of the training data and clusters centers are shown. We show how to make new predictions. A few problems and possible solutions to K-means are explained. Different clusters metrics are used to check the efficiency of K-means, outlier detection is done, and data transformations are used. Finally, DBSCAN is introduced as a solution to most K-means problems. Habilities: K-Means DBSCAN Model Training Model Evaluation Dimension Reduction Algorithms High-dimensional data present a challenge for statistical models. Fortunately, much of the data is redundant and can be reduced to a smaller number of variables without losing much information. Typically, we use dimensionality reduction in machine learning and data exploration. In machine learning, we use it to reduce the number of features. This will decrease computational power and possibly lead to better model performance. Similarly, we can use dimensionality reduction to project data into two dimensions. Such visualization can help us detect outliers or data clusters.\nIn this project, we will compare four different methods to accomplish such a task: (1) Principal Component Analysis (PCA), (2) Kernel Principal Component Analysis (kPCA), (3) Linear Discriminant Analysis (LDA), and (4) t-distributed Stochastic Neighboring Entities (t-SNE). For this, we will use the Iris dataset provided within scikit-learn, which consists of 150 samples, each with 4 features.\nFirst, the distribution of the elements in the datasets is analyzed. We begin using PCA, defining when this algorithm is a good idea to use. Then, the same is done with Kernel PCA, LDA, and t-SNE. Finally, PCA and LDA are compared and used as preprocessing steps of KNN. Habilities: Principal Component Analysis (PCA) Kernel Principal Component Analysis (kPCA) Linear Discriminant Analysis (LDA) t-distributed Stochastic Neighboring Entities (t-SNE) Model Training Model Preprocessing Neural Networks This notebook starts with the visualization of several activation functions. Then, the fashion_mnist Keras dataset is loaded, a dataset containing 15000 images, every pixel is represented with 255 pixels, and there is a total of 9 classes. Several techniques are used to unzip the manually downloaded files. Afterward, the distribution of the dataset is analyzed, and the images are processed to show them. A Multi-Layer Perceptron (MLP) is created to classify the images of this dataset using Keras. Different properties of this model are shown. Then, the model is trained, and information related to the training process is plotted. Finally, we evaluate this image classifier, and some images and its prediction are plotted it. Afterward, MLP is used in a Regression Problem, predicting the prices of houses in the dataset California housing. More complex models are created using the Functional API of Keras, and different tools to save and load Keras models are used. Habilities: Deep Learning MLP Keras Data Visualization Model Training Image Classification Price Prediction Resources GitHub Link\n","date":"July 22, 2022","hero":"/portfolio/posts/work/project9/hero.png","permalink":"https://lorainemg.github.io/portfolio/posts/work/project9/","summary":"My Collection of Projects teaching Machine Learning subject. In this article every project is described.","tags":["Data Manipulation","Data Visualization","Importing \u0026 Cleaning Data","Programming","Probability \u0026 Statistics","Machine Learning"],"title":"Machine Learning Projects"},{"categories":["Basic"],"contents":"My Collection of Projects teaching Machine Learning subject.\nHere\u0026rsquo;s a brief description of every project:\nSupervised Classification and Introduction to the Sklearn Tool The objective of this project is to solve a problem using the algorithms given in the lecture.\nThere are positive and negative movie reviews. 1000 of each. The goal is, given the text of the review, to determine whether the review is positive or negative. For this, KNN and Naive Bayes will be used and compared. Habilities: Data Preprocessing KNN Naive Bayes Feature Extraction Model Training Sentiment Analysis Decision Trees and Random Forest To demonstrate the use of Decision Trees and Random Forests, we use the Iris dataset.\nThe characteristics of the Iris data set are analyzed, visualizing the most important characteristics. The behavior of each algorithm is analyzed, visualizing its decision space. The depth of the trees is adjusted to avoid overfitting. Then, with the Decision Tree and Random Forest algorithms, the dataset used in the previous project, Rotten Tomatoes, is analyzed, where the sentiment analysis is performed. Habilities: Data Preprocessing Decision Trees Random Forest Feature Analysis Model Tuning Model Training Sentiment Analysis Logistic Regression We continue to use the Rotten Tomatoes dataset to rank positive and negative reviews.\nAs in the previous projects, we extract the content of the files, get a bag of words representation of the dataset, and begin to train the model. This time, we visualize the Confusion Matrix to assess the performance of the algorithm and show how to test the prediction of the algorithm in a specific example. In addition, we use the coefficient of the Logistic Regression model to check the most important features. We plot the most important positive and negative words. We check different techniques that would make the model better: add bigrams and decrease the probability threshold of Logistic Regression. Habilities: Data Preprocessing Logistic Regression Feature Analysis Model Tuning Model Training Sentiment Analysis Natural Language Processing Support Vector Machine and Cross Validation We work with a dataset that contains information about patients (gender, marital status, smoking status, age, etc.) with the aim of predicting whether they are likely to have a heart attack (stroke). In addition, the doctor characteristic is artificially incorporated, which represents the doctor who collected the data, which will later be used to group the data.\nWe clean our data encoding some categorical variables and checking for NaN values. We use Support Vector Machine with this dataset to classify patients that could have a heart attack. AUC metric is introduced, good for the imbalanced datasets, like the one we were using. The concept of Baseline is used, to efficiently compare models and different partition methods to use with Cross Validation are visualized. Different algorithms are compared using correct evaluation techniques. Habilities: Data Cleaning Support Vector Machines Model Training Model Evaluation Cross Validation Clustering Algorithms: K-Means y DBSCAN In this project, we explore how K-means and DBSCAN work.\nWe show various K-means issues and provide solutions to address those issues, including cluster quality analysis. We study how to artificially generate datasets with Sklearn, visualizing the data distribution to apply clustering. K-means algorithm is explained, and the predictions of the training data and clusters centers are shown. We show how to make new predictions. A few problems and possible solutions to K-means are explained. Different clusters metrics are used to check the efficiency of K-means, outlier detection is done, and data transformations are used. Finally, DBSCAN is introduced as a solution to most K-means problems. Habilities: K-Means DBSCAN Model Training Model Evaluation Dimension Reduction Algorithms High-dimensional data present a challenge for statistical models. Fortunately, much of the data is redundant and can be reduced to a smaller number of variables without losing much information. Typically, we use dimensionality reduction in machine learning and data exploration. In machine learning, we use it to reduce the number of features. This will decrease computational power and possibly lead to better model performance. Similarly, we can use dimensionality reduction to project data into two dimensions. Such visualization can help us detect outliers or data clusters.\nIn this project, we will compare four different methods to accomplish such a task: (1) Principal Component Analysis (PCA), (2) Kernel Principal Component Analysis (kPCA), (3) Linear Discriminant Analysis (LDA), and (4) t-distributed Stochastic Neighboring Entities (t-SNE). For this, we will use the Iris dataset provided within scikit-learn, which consists of 150 samples, each with 4 features.\nFirst, the distribution of the elements in the datasets is analyzed. We begin using PCA, defining when this algorithm is a good idea to use. Then, the same is done with Kernel PCA, LDA, and t-SNE. Finally, PCA and LDA are compared and used as preprocessing steps of KNN. Habilities: Principal Component Analysis (PCA) Kernel Principal Component Analysis (kPCA) Linear Discriminant Analysis (LDA) t-distributed Stochastic Neighboring Entities (t-SNE) Model Training Model Preprocessing Neural Networks This notebook starts with the visualization of several activation functions. Then, the fashion_mnist Keras dataset is loaded, a dataset containing 15000 images, every pixel is represented with 255 pixels, and there is a total of 9 classes. Several techniques are used to unzip the manually downloaded files. Afterward, the distribution of the dataset is analyzed, and the images are processed to show them. A Multi-Layer Perceptron (MLP) is created to classify the images of this dataset using Keras. Different properties of this model are shown. Then, the model is trained, and information related to the training process is plotted. Finally, we evaluate this image classifier, and some images and its prediction are plotted it. Afterward, MLP is used in a Regression Problem, predicting the prices of houses in the dataset California housing. More complex models are created using the Functional API of Keras, and different tools to save and load Keras models are used. Habilities: Deep Learning MLP Keras Data Visualization Model Training Image Classification Price Prediction Resources GitHub Link\n","date":"July 22, 2022","hero":"/portfolio/projects/work/project9/hero.png","permalink":"https://lorainemg.github.io/portfolio/projects/work/project9/","summary":"\u003cp\u003eMy Collection of Projects teaching Machine Learning subject.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s a brief description of every project:\u003c/p\u003e\n\u003ch4 id=\"supervised-classification-and-introduction-to-the-sklearn-tool\"\u003eSupervised Classification and Introduction to the Sklearn Tool\u003c/h4\u003e\n\u003cp\u003eThe objective of this project is to solve a problem using the algorithms given in the lecture.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThere are positive and negative movie reviews. 1000 of each.\u003c/li\u003e\n\u003cli\u003eThe goal is, given the text of the review, to determine whether the review is positive or negative.\u003c/li\u003e\n\u003cli\u003eFor this, KNN and Naive Bayes will be used and compared.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"habilities\"\u003eHabilities:\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eData Preprocessing\u003c/li\u003e\n\u003cli\u003eKNN\u003c/li\u003e\n\u003cli\u003eNaive Bayes\u003c/li\u003e\n\u003cli\u003eFeature Extraction\u003c/li\u003e\n\u003cli\u003eModel Training\u003c/li\u003e\n\u003cli\u003eSentiment Analysis\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"decision-trees-and-random-forest\"\u003eDecision Trees and Random Forest\u003c/h4\u003e\n\u003cp\u003eTo demonstrate the use of Decision Trees and Random Forests, we use the Iris dataset.\u003c/p\u003e","tags":["Data Manipulation","Data Visualization","Importing \u0026 Cleaning Data","Programming","Probability \u0026 Statistics","Machine Learning"],"title":"Machine Learning Projects"},{"categories":["Basic"],"contents":"The field of Automated Machine Learning (AutoML) has been highlighted as one of the main alternatives to finding good solutions for complex machine learning problems. Despite the recent success of AutoML, many challenges remain. Learning AutoML is a time-consuming process and can be computationally inefficient. Meta-learning is described as the process of learning from past experiences by applying various learning algorithms to different types of data and thus reducing the time needed to learn new tasks. One of the advantages of meta-learning techniques is that they can serve as efficient support for the AutoML process, learning from previous tasks the best algorithms to solve a certain type of problem. In this way, it is possible to speed up the AutoML process, obtaining better results in the same period of time. The objective of this thesis project is to design a meta-learning strategy for generic domains in machine learning.\nSummary The implemented meta-learning proposal is capable of addressing a wide variety of tasks by selecting features capable of representing the space defined by them. AutoGOAL has been replaced as a complementary AutoML system, which stands out for its ability to generate effective solutions for a wide range of domains, allowing you to solve a large number of tasks. AutoGOAL is used for pipeline generation algorithms to create the knowledge base and pipeline search initialized with the designed meta-learning approach.\nThe developed meta-learning approach consists in the selection of a set of algorithm pipelines to be recommended in the initialization of the optimization process of an AutoML system. The choice of this set of pipelines is done by a ranking approach, where for a new dataset the k best algorithm pipelines are selected. For this, several strategies were implemented. Experimental evaluation performed on a large number of datasets shows that these meta-learning strategies performed better in terms of algorithm pipelines found than AutoGOAL without meta-learning, without any consideration of specific domain or problem.\nFirst phase of the meta-learning proposal, were the adquisition of the knowledge is done. Second phase of the meta-learning proposal, were the knowledge of the previous machine learning problems is used to recommend a pipeline for the solution of the current task. Skills Scikit-Learn XGBoost Model Training Model Selection Programming Docker Resources Solution Code GitHub Link Experiments Code GitHub Link Dissertation GitHub Link ","date":"November 17, 2021","hero":"/portfolio/posts/school/project10/hero.png","permalink":"https://lorainemg.github.io/portfolio/posts/school/project10/","summary":"The field of \u003cstrong\u003eAutomated Machine Learning (AutoML)\u003c/strong\u003e has been highlighted as one of the main alternatives to finding good solutions for complex machine learning problems. Meta-learning is described as the process of learning from past experiences by applying various learning algorithms to different types of data and thus reducing the time needed to learn new tasks.The objective of this thesis project is to design a meta-learning strategy for generic domains in AutoML.","tags":["Data Manipulation","Machine Learning","Recommender Systems","AutoML","Programming","Probability \u0026 Statistics","Data Visualization","Hypothesis Testing"],"title":"A Meta-Learning Strategy for Generic AutoML Pipelines."},{"categories":["Basic"],"contents":"Summary The field of Automated Machine Learning (AutoML) has been highlighted as one of the main alternatives to finding good solutions for complex machine learning problems. Despite the recent success of AutoML, many challenges remain. Learning AutoML is a time-consuming process and can be computationally inefficient. Meta-learning is described as the process of learning from past experiences by applying various learning algorithms to different types of data and thus reducing the time needed to learn new tasks. One of the advantages of meta-learning techniques is that they can serve as efficient support for the AutoML process, learning from previous tasks the best algorithms to solve a certain type of problem. In this way, it is possible to speed up the AutoML process, obtaining better results in the same period of time. The objective of this thesis project is to design a meta-learning strategy for generic domains in machine learning.\nThe implemented meta-learning proposal is capable of addressing a wide variety of tasks by selecting features capable of representing the space defined by them. AutoGOAL has been replaced as a complementary AutoML system, which stands out for its ability to generate effective solutions for a wide range of domains, allowing you to solve a large number of tasks. AutoGOAL is used for pipeline generation algorithms to create the knowledge base and pipeline search initialized with the designed meta-learning approach.\nThe developed meta-learning approach consists in the selection of a set of algorithm pipelines to be recommended in the initialization of the optimization process of an AutoML system. The choice of this set of pipelines is done by a ranking approach, where for a new dataset the k best algorithm pipelines are selected. For this, several strategies were implemented. Experimental evaluation performed on a large number of datasets shows that these meta-learning strategies performed better in terms of algorithm pipelines found than AutoGOAL without meta-learning, without any consideration of specific domain or problem.\nFirst phase of the meta-learning proposal, were the adquisition of the knowledge is done. Second phase of the meta-learning proposal, were the knowledge of the previous machine learning problems is used to recommend a pipeline for the solution of the current task. Skills Scikit-Learn XGBoost Model Training Model Selection Programming Docker Resources Solution Code GitHub Link Experiments Code GitHub Link Dissertation GitHub Link ","date":"November 17, 2021","hero":"/portfolio/projects/school/project10/hero.png","permalink":"https://lorainemg.github.io/portfolio/projects/school/project10/","summary":"\u003ch4 id=\"summary\"\u003eSummary\u003c/h4\u003e\n\u003cp\u003eThe field of \u003cstrong\u003eAutomated Machine Learning (AutoML)\u003c/strong\u003e has been highlighted as one of the main alternatives to finding good solutions for complex machine learning problems. Despite the recent success of AutoML, many challenges remain. Learning AutoML is a time-consuming process and can be computationally inefficient. Meta-learning is described as the process of learning from past experiences by applying various learning algorithms to different types of data and thus reducing the time needed to learn new tasks. One of the advantages of meta-learning techniques is that they can serve as efficient support for the AutoML process, learning from previous tasks the best algorithms to solve a certain type of problem. In this way, it is possible to speed up the AutoML process, obtaining better results in the same period of time. The objective of this thesis project is to design a meta-learning strategy for generic domains in machine learning.\u003c/p\u003e","tags":["Data Manipulation","Machine Learning","Recommender Systems","AutoML","Programming","Probability \u0026 Statistics","Data Visualization","Hypothesis Testing"],"title":"A Meta-Learning Strategy for Generic AutoML Pipelines."},{"categories":["Basic"],"contents":"This is the solution presented by the UH-MMM group to the eHealth-KD challenge at IberLEF 2021.\nSummary Two main subtasks for knowledge discovery were defined: entity recognition and relationship extraction. The evaluation of the task is divided into three scenarios: one corresponding to the detection of entities, one corresponding to the detection of relations between such pair of entities, and the third one corresponding to the extraction of both entities and relationships. For both subtasks, our proposal makes use of BiLSTM as contextual encoders and Dense layers as the tag decoder architecture of the model. In the challenge, the system ranked fifth in the main scenario, fourth in the scenario evaluating the first task, and fifth in the last scenario. The score obtained in the relationship extraction task shows that the proposed approach needs to be further explored. Neural network architecture used in the first subtask Skills Scikit-learn Named Entity Recognition Relation Extraction Keras NLTK Spacy Neural Networks LSTM Resouces GitHub Link Paper Challenge Link ","date":"July 20, 2021","hero":"/portfolio/posts/school/project7/hero.png","permalink":"https://lorainemg.github.io/portfolio/posts/school/project7/","summary":"This is the solution presented by the UH-MMM group to the eHealth-KD challenge at IberLEF 2021 at the 3rd Iberian Languages Evaluation Forum.","tags":["Natural Language Processing","Machine Learning","Deep Learning","Importing \u0026 Cleaning Data","Programming"],"title":"eHealth-KD Challenge 2021"},{"categories":["Basic"],"contents":"Summary This is the solution presented by the UH-MMM group to the eHealth-KD challenge at IberLEF 2021.\nTwo main subtasks for knowledge discovery were defined: entity recognition and relationship extraction. The evaluation of the task is divided into three scenarios: one corresponding to the detection of entities, one corresponding to the detection of relations between such pair of entities, and the third one corresponding to the extraction of both entities and relationships. For both subtasks, our proposal makes use of BiLSTM as contextual encoders and Dense layers as the tag decoder architecture of the model. In the challenge, the system ranked fifth in the main scenario, fourth in the scenario evaluating the first task, and fifth in the last scenario. The score obtained in the relationship extraction task shows that the proposed approach needs to be further explored. Neural network architecture used in the first subtask Skills Scikit-learn Named Entity Recognition Relation Extraction Keras NLTK Spacy Neural Networks LSTM Resouces GitHub Link Paper Challenge Link ","date":"July 20, 2021","hero":"/portfolio/projects/school/project7/hero.png","permalink":"https://lorainemg.github.io/portfolio/projects/school/project7/","summary":"\u003ch4 id=\"summary\"\u003eSummary\u003c/h4\u003e\n\u003cp\u003eThis is the solution presented by the UH-MMM group to the eHealth-KD challenge at IberLEF 2021.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTwo main subtasks for knowledge discovery were defined: entity recognition and relationship extraction.\u003c/li\u003e\n\u003cli\u003eThe evaluation of the task is divided into three scenarios: one corresponding to the detection of entities, one corresponding to the detection of relations between such pair of entities, and the third one corresponding to the extraction of both entities and relationships.\u003c/li\u003e\n\u003cli\u003eFor both subtasks, our proposal makes use of BiLSTM as contextual encoders and Dense layers as the tag decoder architecture of the model.\u003c/li\u003e\n\u003cli\u003eIn the challenge, the system ranked fifth in the main scenario, fourth in the scenario evaluating the first task, and fifth in the last scenario.\u003c/li\u003e\n\u003cli\u003eThe score obtained in the relationship extraction task shows that the proposed approach needs to be further explored.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure\u003e\u003cimg src=\"/portfolio/projects/school/project7/project7.jpg\"\u003e\u003cfigcaption\u003e\n      \u003ch4\u003eNeural network architecture used in the first subtask\u003c/h4\u003e\n    \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch4 id=\"skills\"\u003e\u003cstrong\u003eSkills\u003c/strong\u003e\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eScikit-learn\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eNamed Entity Recognition\u003c/li\u003e\n\u003cli\u003eRelation Extraction\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eKeras\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eNLTK\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSpacy\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eNeural Networks\u003c/li\u003e\n\u003cli\u003eLSTM\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"resouces\"\u003eResouces\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lorainemg/eHealthKD-competition\" target=\"_blank\" rel=\"noopener\"\u003eGitHub Link\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lorainemg/eHealthKD-competition/blob/main/docs/ehealth_paper4.pdf\" target=\"_blank\" rel=\"noopener\"\u003ePaper\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ehealthkd.github.io/2021\" target=\"_blank\" rel=\"noopener\"\u003eChallenge Link\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":["Natural Language Processing","Machine Learning","Deep Learning","Importing \u0026 Cleaning Data","Programming"],"title":"eHealth-KD Challenge 2021"},{"categories":["Basic"],"contents":"Scraper of Sensortower to collect app download statistics for Android and iOS.\nSummary It collects information from Sensortower API. Saves the necessary information in JSON. It uses the Python requests package. Ejemplo de la información obtenida en Sensortower Skills Scrapper API JSON requests Resources GitHub Link ","date":"July 1, 2020","hero":"/portfolio/posts/personal/project6/hero.png","permalink":"https://lorainemg.github.io/portfolio/posts/personal/project6/","summary":"Scraper of \u003ca href=\"https://app.sensortower.com/\" target=\"_blank\" rel=\"noopener\"\u003eSensortower\u003c/a\u003e to collect app download statistics for Android and iOS.","tags":["Data Manipulation","Importing \u0026 Cleaning Data","Programming","API"],"title":"Sensortower Scraper"},{"categories":["Basic"],"contents":"Summary Scraper of Sensortower to collect app download statistics for Android and iOS.\nIt collects information from Sensortower API. Saves the necessary information in JSON. It uses the Python requests package. Ejemplo de la información obtenida en Sensortower Skills Scrapper API JSON requests Resources GitHub Link ","date":"July 1, 2020","hero":"/portfolio/projects/personal/project6/hero.png","permalink":"https://lorainemg.github.io/portfolio/projects/personal/project6/","summary":"\u003ch4 id=\"summary\"\u003eSummary\u003c/h4\u003e\n\u003cp\u003eScraper of \u003ca href=\"https://app.sensortower.com/\" target=\"_blank\" rel=\"noopener\"\u003eSensortower\u003c/a\u003e to collect app download statistics for Android and iOS.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt collects information from \u003ca href=\"https://sensortower.com/api/\" target=\"_blank\" rel=\"noopener\"\u003eSensortower API\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eSaves the necessary information in JSON.\u003c/li\u003e\n\u003cli\u003eIt uses the Python \u003ccode\u003erequests\u003c/code\u003e package.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure\u003e\u003cimg src=\"/portfolio/projects/personal/project6/project6.jpg\"\u003e\u003cfigcaption\u003e\n      \u003ch4\u003eEjemplo de la información obtenida en Sensortower\u003c/h4\u003e\n    \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch4 id=\"skills\"\u003e\u003cstrong\u003eSkills\u003c/strong\u003e\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eScrapper\u003c/li\u003e\n\u003cli\u003eAPI\u003c/li\u003e\n\u003cli\u003eJSON\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erequests\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"resources\"\u003eResources\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lorainemg/sensortower-scraper\" target=\"_blank\" rel=\"noopener\"\u003eGitHub Link\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":["Data Manipulation","Importing \u0026 Cleaning Data","Programming","API"],"title":"Sensortower Scraper"},{"categories":["Basic"],"contents":"In this project, an Information Retrieval System based on the Vector Information Retrieval Model is implemented. The design of the system is exposed according to each stage of information retrieval and several of its functionalities are implemented.\nSummary A visual interface using Streamlit is used. Clustering techniques to group documents and a crawler to gain information are applied. NLP techniques and different metrics in different collections are applied to check the functionality of the system. Workflow of an Information Retrieval System Skills Machine Learning Clustering Content Filtering Streamlit NLTK Scikit-Learn Resources GitHub Link Report ","date":"June 18, 2020","hero":"/portfolio/posts/school/project3/hero.png","permalink":"https://lorainemg.github.io/portfolio/posts/school/project3/","summary":"In this project, an Information Retrieval System based on the Vector Information Retrieval Model is implemented. The design of the system is exposed according to each stage of information retrieval and several of its functionalities are implemented.","tags":["Information Retrieval","Recommender Systems","NLP","Data Manipulation","Importing \u0026 Cleaning Data","Machine Learning"],"title":"Information Retrieval System"},{"categories":["Basic"],"contents":"Summary In this repo, an Information Retrieval System based on the Vector Information Retrieval Model is implemented. The design of the system is exposed according to each stage of information retrieval and several of its functionalities are implemented. A visual interface using Streamlit is used. Clustering techniques to group documents and a crawler to gain information are applied. NLP techniques and different metrics in different collections are applied to check the functionality of the system. Workflow of an Information Retrieval System Skills Machine Learning Clustering Content Filtering Streamlit NLTK Scikit-Learn Resources GitHub Link Report ","date":"June 18, 2020","hero":"/portfolio/projects/school/project3/hero.png","permalink":"https://lorainemg.github.io/portfolio/projects/school/project3/","summary":"\u003ch4 id=\"summary\"\u003eSummary\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eIn this repo, an Information Retrieval System based on the Vector Information Retrieval Model is implemented.\u003c/li\u003e\n\u003cli\u003eThe design of the system is exposed according to each stage of information retrieval and several of its functionalities are implemented.\u003c/li\u003e\n\u003cli\u003eA visual interface using Streamlit is used.\u003c/li\u003e\n\u003cli\u003eClustering techniques to group documents and a crawler to gain information are applied.\u003c/li\u003e\n\u003cli\u003eNLP techniques and different metrics in different collections are applied to check the functionality of the system.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure\u003e\u003cimg src=\"/portfolio/projects/school/project3/project3.jpg\"\u003e\u003cfigcaption\u003e\n      \u003ch4\u003eWorkflow of an Information Retrieval System\u003c/h4\u003e\n    \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch4 id=\"skills\"\u003e\u003cstrong\u003eSkills\u003c/strong\u003e\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eMachine Learning\u003c/li\u003e\n\u003cli\u003eClustering\u003c/li\u003e\n\u003cli\u003eContent Filtering\u003c/li\u003e\n\u003cli\u003eStreamlit\u003c/li\u003e\n\u003cli\u003eNLTK\u003c/li\u003e\n\u003cli\u003eScikit-Learn\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"resources\"\u003eResources\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lorainemg/information-retrieval-system\" target=\"_blank\" rel=\"noopener\"\u003eGitHub Link\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lorainemg/information-retrieval-system/blob/main/doc/C511%20Loraine%20Monteagudo%2C%20Tony%20Raul%20Blanco.pdf\" target=\"_blank\" rel=\"noopener\"\u003eReport\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":["Information Retrieval","Recommender Systems","NLP","Data Manipulation","Importing \u0026 Cleaning Data","Machine Learning"],"title":"Information Retrieval System"},{"categories":["Basic"],"contents":"This project uses the data from the UCI Machine Learning Repository, in particular the dataset \u0026ldquo;Individual Household Electric Power Consumption\u0026rdquo;. A real example related to the electricity consumption of a house for 4 years is analyzed.\nSummary The project is written in R. A normal population will be generated in which different samples will be extracted and their differences will be studied. Then, we will carry out a hypothesis test of equality of variances between two attributes of the dataset. In addition, a study of these data will be carried out using regression techniques, which will allow us to determine if there are linear relationships between some of the given variables. Then we will reduce the dimension of the data through principal component analysis (PCA), clusters and classification trees, also making an interpretation of them. Finally, we will carry out an analysis of variance or ANOVA to compare the means of a characteristic in several populations. Graphic of the correlation matrix between several of the variables analyzed in the data set Skills R Clustering Statistical Analysis PCA Analysis Hypothesis Testing Regression Analysis Anova Test Resources Github Link Report 1 Report 2 Dataset ","date":"September 25, 2019","hero":"/portfolio/posts/school/project2/hero.png","permalink":"https://lorainemg.github.io/portfolio/posts/school/project2/","summary":"This project uses the data from the UCI Machine Learning Repository, in particular the dataset \u0026ldquo;Individual Household Electric Power Consumption\u0026rdquo;. A real example related to the electricity consumption of a house for 4 years is analyzed.","tags":["Data Analysis","Data Visualization","Data Manipulation","Probability \u0026 Statistics","Hypothesis Testing"],"title":"Analysis of Household Dataset"},{"categories":["Basic"],"contents":"Summary This project uses the data from the UCI Machine Learning Repository, in particular the dataset \u0026ldquo;Individual Household Electric Power Consumption\u0026rdquo;. A real example related to the electricity consumption of a house for 4 years is analyzed.\nThe project is written in R. A normal population will be generated in which different samples will be extracted and their differences will be studied. Then, we will carry out a hypothesis test of equality of variances between two attributes of the dataset. In addition, a study of these data will be carried out using regression techniques, which will allow us to determine if there are linear relationships between some of the given variables. Then we will reduce the dimension of the data through principal component analysis (PCA), clusters and classification trees, also making an interpretation of them. Finally, we will carry out an analysis of variance or ANOVA to compare the means of a characteristic in several populations. Graphic of the correlation matrix between several of the variables analyzed in the data set Skills R Clustering Statistical Analysis PCA Analysis Hypothesis Testing Regression Analysis Anova Test Resources Github Link Report 1 Report 2 Dataset ","date":"September 25, 2019","hero":"/portfolio/projects/school/project2/hero.png","permalink":"https://lorainemg.github.io/portfolio/projects/school/project2/","summary":"\u003ch4 id=\"summary\"\u003eSummary\u003c/h4\u003e\n\u003cp\u003eThis project uses the data from the UCI Machine Learning Repository, in particular the dataset \u0026ldquo;Individual Household Electric Power Consumption\u0026rdquo;. A real example related to the electricity consumption of a house for 4 years is analyzed.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe project is written in R.\u003c/li\u003e\n\u003cli\u003eA normal population will be generated in which different samples will be extracted and their differences will be studied.\u003c/li\u003e\n\u003cli\u003eThen, we will carry out a hypothesis test of equality of variances between two attributes of the dataset.\u003c/li\u003e\n\u003cli\u003eIn addition, a study of these data will be carried out using regression techniques, which will allow us to determine if there are linear relationships between some of the given variables.\u003c/li\u003e\n\u003cli\u003eThen we will reduce the dimension of the data through principal component analysis (PCA), clusters and classification trees, also making an interpretation of them.\u003c/li\u003e\n\u003cli\u003eFinally, we will carry out an analysis of variance or ANOVA to compare the means of a characteristic in several populations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure\u003e\u003cimg src=\"/portfolio/projects/school/project2/project2.png\"\u003e\u003cfigcaption\u003e\n      \u003ch4\u003eGraphic of the correlation matrix between several of the variables analyzed in the data set\u003c/h4\u003e\n    \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n\u003ch4 id=\"skills\"\u003e\u003cstrong\u003eSkills\u003c/strong\u003e\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eR\u003c/li\u003e\n\u003cli\u003eClustering\u003c/li\u003e\n\u003cli\u003eStatistical Analysis\u003c/li\u003e\n\u003cli\u003ePCA Analysis\u003c/li\u003e\n\u003cli\u003eHypothesis Testing\u003c/li\u003e\n\u003cli\u003eRegression Analysis\u003c/li\u003e\n\u003cli\u003eAnova Test\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"resources\"\u003eResources\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lorainemg/Household-Analysis\" target=\"_blank\" rel=\"noopener\"\u003eGithub Link\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lorainemg/Household-Analysis/blob/main/Phase1/doc/report.pdf\" target=\"_blank\" rel=\"noopener\"\u003eReport 1\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lorainemg/Household-Analysis/blob/main/Phase2/doc/report.pdf\" target=\"_blank\" rel=\"noopener\"\u003eReport 2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://archive.ics.uci.edu/ml/datasets/individual\u0026#43;household\u0026#43;electric\u0026#43;power\u0026#43;consumption\" target=\"_blank\" rel=\"noopener\"\u003eDataset\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":["Data Analysis","Data Visualization","Data Manipulation","Probability \u0026 Statistics","Hypothesis Testing"],"title":"Analysis of Household Dataset"},{"categories":null,"contents":"This section highlights projects I have developed in a professional capacity. These projects demonstrate my ability to apply my skills in real-world scenarios, addressing specific business needs and contributing to organizational goals.\n","date":"September 2, 2019","hero":"/portfolio/images/default-hero.jpg","permalink":"https://lorainemg.github.io/portfolio/posts/work/_index.bn/","summary":"\u003cp\u003eThis section highlights projects I have developed in a professional capacity. These projects demonstrate my ability to apply my skills in real-world scenarios, addressing specific business needs and contributing to organizational goals.\u003c/p\u003e","tags":null,"title":"Work Projects"},{"categories":null,"contents":"This section highlights projects I have developed in a professional capacity. These projects demonstrate my ability to apply my skills in real-world scenarios, addressing specific business needs and contributing to organizational goals.\n","date":"September 2, 2019","hero":"/portfolio/images/default-hero.jpg","permalink":"https://lorainemg.github.io/portfolio/projects/work/_index.bn/","summary":"\u003cp\u003eThis section highlights projects I have developed in a professional capacity. These projects demonstrate my ability to apply my skills in real-world scenarios, addressing specific business needs and contributing to organizational goals.\u003c/p\u003e","tags":null,"title":"Work Projects"},{"categories":["Basic"],"contents":"Schizophrenia is a psychiatric disorder that affects a large part of the world’s population. Among its symptoms are the inability to process information, mainly in attention tasks and work memory. That is why the use of evoked potentials related to events is a useful tool to support the subjective decision processes of medical specialists. The objective of this work is to generate a model that allows distinguishing schizophrenic patients from healthy patients using the visual paradigm of the P300 and applying the discrete transform of wavelets as a method of extracting characteristics.\nSummary The database used has records of evoked potentials of 54 healthy subjects and 54 patients matched by age and sex. The discrete wavelet transform was applied with the Daubechies mother wavelet of order 4 and level 5. 180 characteristics were extracted per subject and SVM was applied as a learning algorithm using cross-validation to obtain a 62.93 % accuracy. Skills Machine Learning SVM Time-Series Analysis Scikit-Learn Wavelet Transform Evoked Potential Resources GitHub Link ","date":"May 9, 2018","hero":"/portfolio/posts/school/project1/hero.png","permalink":"https://lorainemg.github.io/portfolio/posts/school/project1/","summary":"The objective of this work is to generate a model that allows distinguishing schizophrenic patients from healthy patients using the visual paradigm of the P300 and applying the discrete transform of wavelets as a method of extracting characteristics.","tags":["Data Modeling","Machine Learning","Data Manipulation","Importing \u0026 Cleaning Data"],"title":"Schizophrenic Classification"},{"categories":["Basic"],"contents":"Summary Schizophrenia is a psychiatric disorder that affects a large part of the world’s population. Among its symptoms are the inability to process information, mainly in attention tasks and work memory. That is why the use of evoked potentials related to events is a useful tool to support the subjective decision processes of medical specialists.\nThe objective of this work is to generate a model that allows distinguishing schizophrenic patients from healthy patients using the visual paradigm of the P300 and applying the discrete transform of wavelets as a method of extracting characteristics. The database used has records of evoked potentials of 54 healthy subjects and 54 patients matched by age and sex. The discrete wavelet transform was applied with the Daubechies mother wavelet of order 4 and level 5. 180 characteristics were extracted per subject and SVM was applied as a learning algorithm using cross-validation to obtain a 62.93 % accuracy. Skills Machine Learning SVM Time-Series Analysis Scikit-Learn Wavelet Transform Evoked Potential Resources GitHub Link ","date":"May 9, 2018","hero":"/portfolio/projects/school/project1/hero.png","permalink":"https://lorainemg.github.io/portfolio/projects/school/project1/","summary":"\u003ch4 id=\"summary\"\u003eSummary\u003c/h4\u003e\n\u003cp\u003eSchizophrenia is a psychiatric disorder that affects a large part of the world’s population. Among its symptoms are the inability to process information, mainly in attention tasks and work memory. That is why the use of evoked potentials related to events is a useful tool to support the subjective decision processes of medical specialists.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe objective of this work is to generate a model that allows distinguishing schizophrenic patients from healthy patients using the visual paradigm of the P300 and applying the discrete transform of wavelets as a method of extracting characteristics.\u003c/li\u003e\n\u003cli\u003eThe database used has records of evoked potentials of 54 healthy subjects and 54 patients matched by age and sex.\u003c/li\u003e\n\u003cli\u003eThe discrete wavelet transform was applied with the Daubechies mother wavelet of order 4 and level 5.\u003c/li\u003e\n\u003cli\u003e180 characteristics were extracted per subject and SVM was applied as a learning algorithm using cross-validation to obtain a 62.93 % accuracy.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cimg src=\"project1.png\" alt=\"Fourier Transform\" style=\"width:100%;\" title=\"Calculation of the Fast Fourier Transform of the reconstructed signal from the A5 coefficients that represents the Delta band [0.5-4Hz]\"/\u003e\n\u003ch4 id=\"skills\"\u003e\u003cstrong\u003eSkills\u003c/strong\u003e\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eMachine Learning\u003c/li\u003e\n\u003cli\u003eSVM\u003c/li\u003e\n\u003cli\u003eTime-Series Analysis\u003c/li\u003e\n\u003cli\u003eScikit-Learn\u003c/li\u003e\n\u003cli\u003eWavelet Transform\u003c/li\u003e\n\u003cli\u003eEvoked Potential\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"resources\"\u003eResources\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lorainemg/schizophrenic-classification\" target=\"_blank\" rel=\"noopener\"\u003eGitHub Link\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":["Data Modeling","Machine Learning","Data Manipulation","Importing \u0026 Cleaning Data"],"title":"Schizophrenic Classification"},{"categories":["Basic"],"contents":"The automatic detection of negation and the words they affect is an important task that could benefit other Natural Language Processing tasks such as Information Extraction, Sentiment Analysis, and Question Answering. In the present work, a solution to the negation problem is proposed based on supervised machine learning techniques. The essence is the detection of the negation signals and their scope, all of it in Spanish texts. The work is done in two phases: in the first one the signals are identified and in the second the whole scope of these cues is determined.\nSummary In the first phase it is determined if a certain sentence is negated or not, identifying the words that determine the negation. These predictions are used in the second phase, where the scope of the negation is determined. First, the text is processed and lexical (tokens, part-of-speech tags and lemmas) and syntactic (dependency analysis) information is obtained from it. These data are converted for the input of the negation detector. After obtaining the prediction these are used for the determination of the scope of the negation. A Streamlit application is developed to test the results of the machine learning algorithm. The application is containerized using Docker. Skills NLTK Spacy Streamlit Natural Language Processing Logistic Regression Docker Resources GitHub Link Paper Corpus ","date":"April 10, 2018","hero":"/portfolio/posts/school/project5/hero.png","permalink":"https://lorainemg.github.io/portfolio/posts/school/project5/","summary":"The automatic detection of negation and the words they affect is an important task that could benefit other Natural Language Processing tasks such as Information Extraction, Sentiment Analysis, and Question Answering. In the present work, a solution to the negation problem is proposed based on supervised machine learning techniques. The essence is the detection of the negation signals and their scope, all of it in Spanish texts. The work is done in two phases in the first one the signals are identified and in the second the whole scope of these cues is determined.","tags":["Data Manipulation","Machine Learning","Natural Language Processing","Sentiment Analysis","Programming","Probability \u0026 Statistics"],"title":"Detection of Negation"},{"categories":["Basic"],"contents":"Summary The automatic detection of negation and the words they affect is an important task that could benefit other Natural Language Processing tasks such as Information Extraction, Sentiment Analysis, and Question Answering. In the present work, a solution to the negation problem is proposed based on supervised machine learning techniques. The essence is the detection of the negation signals and their scope, all of it in Spanish texts. The work is done in two phases: in the first one the signals are identified and in the second the whole scope of these cues is determined.\nIn the first phase it is determined if a certain sentence is negated or not, identifying the words that determine the negation. These predictions are used in the second phase, where the scope of the negation is determined. First, the text is processed and lexical (tokens, part-of-speech tags and lemmas) and syntactic (dependency analysis) information is obtained from it. These data are converted for the input of the negation detector. After obtaining the prediction these are used for the determination of the scope of the negation. A Streamlit application is developed to test the results of the machine learning algorithm. The application is containerized using Docker. Skills NLTK Spacy Streamlit Natural Language Processing Logistic Regression Docker Resources GitHub Link Paper Corpus ","date":"April 10, 2018","hero":"/portfolio/projects/school/project5/hero.png","permalink":"https://lorainemg.github.io/portfolio/projects/school/project5/","summary":"\u003ch4 id=\"summary\"\u003eSummary\u003c/h4\u003e\n\u003cp\u003eThe automatic detection of negation and the words they affect is an important task that could benefit other Natural Language Processing tasks such as Information Extraction, Sentiment Analysis, and Question Answering. In the present work, a solution to the negation problem is proposed based on supervised machine learning techniques. The essence is the detection of the negation signals and their scope, all of it in Spanish texts. The work is done in two phases: in the first one the signals are identified and in the second the whole scope of these cues is determined.\u003c/p\u003e","tags":["Data Manipulation","Machine Learning","Natural Language Processing","Sentiment Analysis","Programming","Probability \u0026 Statistics"],"title":"Detection of Negation"},{"categories":null,"contents":"This section features a selection of personal projects that I have undertaken independently. These projects illustrate my skills, interests, and problem-solving abilities across various domains.\n","date":"September 2, 2015","hero":"/portfolio/images/default-hero.jpg","permalink":"https://lorainemg.github.io/portfolio/posts/personal/_index.bn/","summary":"\u003cp\u003eThis section features a selection of personal projects that I have undertaken independently. These projects illustrate my skills, interests, and problem-solving abilities across various domains.\u003c/p\u003e","tags":null,"title":"Personal Projects"},{"categories":null,"contents":"This section features a selection of personal projects that I have undertaken independently. These projects illustrate my skills, interests, and problem-solving abilities across various domains.\n","date":"September 2, 2015","hero":"/portfolio/images/default-hero.jpg","permalink":"https://lorainemg.github.io/portfolio/projects/personal/_index.bn/","summary":"\u003cp\u003eThis section features a selection of personal projects that I have undertaken independently. These projects illustrate my skills, interests, and problem-solving abilities across various domains.\u003c/p\u003e","tags":null,"title":"Personal Projects"},{"categories":null,"contents":"This section showcases a range of projects developed during my time at university. These projects span various disciplines and reflect the depth of learning, research, and practical application I engaged in throughout my academic journey.\n","date":"September 2, 2015","hero":"/portfolio/images/default-hero.jpg","permalink":"https://lorainemg.github.io/portfolio/posts/school/_index.bn/","summary":"\u003cp\u003eThis section showcases a range of projects developed during my time at university. These projects span various disciplines and reflect the depth of learning, research, and practical application I engaged in throughout my academic journey.\u003c/p\u003e","tags":null,"title":"School Projects"},{"categories":null,"contents":"This section showcases a range of projects developed during my time at university. These projects span various disciplines and reflect the depth of learning, research, and practical application I engaged in throughout my academic journey.\n","date":"September 2, 2015","hero":"/portfolio/images/default-hero.jpg","permalink":"https://lorainemg.github.io/portfolio/projects/school/_index.bn/","summary":"\u003cp\u003eThis section showcases a range of projects developed during my time at university. These projects span various disciplines and reflect the depth of learning, research, and practical application I engaged in throughout my academic journey.\u003c/p\u003e","tags":null,"title":"School Projects"},{"categories":null,"contents":"Hi! I am a Computer Scientist graduate from Havana, Cuba:\n😄 I enjoy 💻coding, 📚learning and ⚗️researching. ⚡ In my free time I enjoy reading. I\u0026rsquo;m passionate about Machine Learning and Artificial Intelligence. This is a selection of some of my projects and their descriptions.\n","date":"November 17, 2001","hero":"/portfolio/images/default-hero.jpg","permalink":"https://lorainemg.github.io/portfolio/posts/_index.bn/","summary":"\u003cp\u003eHi! I am a Computer Scientist graduate from Havana, Cuba:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e😄 I enjoy 💻coding, 📚learning and ⚗️researching.\u003c/li\u003e\n\u003cli\u003e⚡ In my free time I enjoy reading. I\u0026rsquo;m passionate about Machine Learning and Artificial Intelligence.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a selection of some of my projects and their descriptions.\u003c/p\u003e","tags":null,"title":"👩‍💻 About me"},{"categories":null,"contents":"Hi! I am a Computer Scientist graduate from Havana, Cuba:\n😄 I enjoy 💻coding, 📚learning and ⚗️researching. ⚡ In my free time I enjoy reading. I\u0026rsquo;m passionate about Machine Learning and Artificial Intelligence. This is a selection of some of my projects and their descriptions.\n","date":"November 17, 2001","hero":"/portfolio/images/default-hero.jpg","permalink":"https://lorainemg.github.io/portfolio/projects/_index.bn/","summary":"\u003cp\u003eHi! I am a Computer Scientist graduate from Havana, Cuba:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e😄 I enjoy 💻coding, 📚learning and ⚗️researching.\u003c/li\u003e\n\u003cli\u003e⚡ In my free time I enjoy reading. I\u0026rsquo;m passionate about Machine Learning and Artificial Intelligence.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is a selection of some of my projects and their descriptions.\u003c/p\u003e","tags":null,"title":"👩‍💻 About me"}]